#!/usr/bin/env python
"""
SAM Prompt Adapter æ¨ç†è„šæœ¬
åŸºäºsam_prompt_train_A+B_new.pyè®­ç»ƒçš„æ¨¡å‹è¿›è¡Œæ¨ç†
"""

import argparse
import os
import sys
import yaml
import cv2
import torch
import numpy as np
from tqdm import tqdm
from PIL import Image
import matplotlib.pyplot as plt
import glob
from pathlib import Path

# æŠ‘åˆ¶matplotlibå­—ä½“è­¦å‘Š
import warnings
warnings.filterwarnings("ignore", category=UserWarning, module="matplotlib")

# å¯¼å…¥æ•°æ®å¤„ç†æ¨¡å—
sys.path.append('/raid/Data/huangtao/tangzhice/matting/baseline_A/data')
from data_processor import DataProcessor

# è®¾å¤‡è®¾ç½®ï¼ˆä½¿ç”¨base.pyé…ç½®ï¼‰
sys.path.append('/raid/Data/huangtao/tangzhice/matting/baseline_A')
from base.base import setup_devices
setup_devices()  # è®¾ç½®ä½¿ç”¨GPU 3
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ===== æ¨ç†é…ç½® =====
# ä¿®æ”¹ä»¥ä¸‹è·¯å¾„ä»¥åŒ¹é…æ‚¨çš„éœ€æ±‚
CHECKPOINT_PATH = "/raid/Data/huangtao/tangzhice/matting/baseline_A/test_out/baseline_A+B/checkpoint_last.pth"
INPUT_PATH = "/raid/Data/huangtao/public/LNSM/test/image"  # è¾“å…¥å›¾åƒç›®å½•
MASK_PATH = "/raid/Data/huangtao/public/LNSM/test/binarymask"  # Binary maskç›®å½•  
OUTPUT_PATH = "/raid/Data/huangtao/tangzhice/matting/baseline_A/test_out/inference_out"  # è¾“å‡ºç›®å½•
VISUALIZE = True  # æ˜¯å¦æ˜¾ç¤ºå¯è§†åŒ–ç»“æœ
BATCH_MODE = True  # æ˜¯å¦æ‰¹é‡å¤„ç†æ¨¡å¼
# ==================


def load_model(checkpoint_path):
    """
    åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    
    Args:
        checkpoint_path: æ¨¡å‹checkpointè·¯å¾„
        
    Returns:
        model: åŠ è½½çš„æ¨¡å‹
    """
    # å¯¼å…¥æ¨¡å‹
    current_dir = os.path.dirname(os.path.abspath(__file__))
    sys.path.append(current_dir)
    
    try:
        from sam_prompt_adapter import SAMAdapterPrompt
    except ImportError as e:
        print(f"Import error: {e}")
        # å°è¯•å…¶ä»–å¯¼å…¥æ–¹å¼
        import importlib.util
        spec = importlib.util.spec_from_file_location("sam_prompt_adapter", 
                                                     os.path.join(current_dir, "sam_prompt_adapter.py"))
        module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(module)
        SAMAdapterPrompt = module.SAMAdapterPrompt
        
        # åˆ›å»ºæ¨¡å‹
    model = SAMAdapterPrompt().to(device)
    
    # åŠ è½½checkpoint
    if checkpoint_path and os.path.exists(checkpoint_path):
        print(f"Loading checkpoint: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=device)
        
        # å¤„ç†ä¸åŒçš„checkpointæ ¼å¼
        if 'model_state_dict' in checkpoint:
            model_state_dict = checkpoint['model_state_dict']
        else:
            model_state_dict = checkpoint
        
        # åŠ è½½æƒé‡
        model.load_state_dict(model_state_dict, strict=False)
        print("âœ… Checkpoint loaded successfully")
    else:
        print(f"âŒ Checkpoint not found: {checkpoint_path}")
        raise FileNotFoundError(f"Checkpoint file not found: {checkpoint_path}")
    
    model.eval()
    return model


def load_image(image_path, target_size=(256, 256)):
    """
    åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ
    
    Args:
        image_path: å›¾åƒè·¯å¾„
        target_size: ç›®æ ‡å°ºå¯¸
        
    Returns:
        image_tensor: é¢„å¤„ç†åçš„å›¾åƒtensor
        original_size: åŸå§‹å›¾åƒå°ºå¯¸
    """
    # è¯»å–å›¾åƒ
    if isinstance(image_path, str):
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"Cannot load image: {image_path}")
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    else:
        image = image_path
    
    original_size = image.shape[:2]
    
    # è°ƒæ•´å°ºå¯¸
    image = cv2.resize(image, target_size, interpolation=cv2.INTER_LINEAR)
    
    # è½¬æ¢ä¸ºtensorå¹¶å½’ä¸€åŒ–
    image_tensor = torch.from_numpy(image.astype(np.float32) / 255.0)
    image_tensor = image_tensor.permute(2, 0, 1).unsqueeze(0)  # [1, 3, H, W]
    
    return image_tensor.to(device), original_size


def load_mask(mask_path, target_size=(256, 256)):
    """
    åŠ è½½å’Œé¢„å¤„ç†binary mask
    
    Args:
        mask_path: binary maskè·¯å¾„
        target_size: ç›®æ ‡å°ºå¯¸
        
    Returns:
        mask_tensor: é¢„å¤„ç†åçš„mask tensor
        trimap_tensor: ä»maskç”Ÿæˆçš„trimap tensorï¼ˆç”¨äºsample_mapï¼‰
    """
    if mask_path and os.path.exists(mask_path):
        # è¯»å–binary mask
        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
        if mask is None:
            raise ValueError(f"Cannot load mask: {mask_path}")
    else:
        # å¦‚æœæ²¡æœ‰maskï¼Œåˆ›å»ºå…¨å‰æ™¯çš„mask
        print("âš ï¸  No mask provided, using full foreground mask")
        mask = np.ones(target_size, dtype=np.uint8) * 255
    
    # è°ƒæ•´å°ºå¯¸
    mask = cv2.resize(mask, target_size, interpolation=cv2.INTER_NEAREST)
    
    # è½¬æ¢binary maskä¸ºfloatå¹¶å½’ä¸€åŒ–åˆ°[0,1]
    mask_normalized = mask.astype(np.float32) / 255.0
    
    # è½¬æ¢ä¸ºtensor
    mask_tensor = torch.from_numpy(mask_normalized).unsqueeze(0).unsqueeze(0)  # [1, 1, H, W]
    
    # ä»binary maskç”Ÿæˆtrimapï¼ˆç”¨äºsample_mapç”Ÿæˆï¼‰
    # binary maskä¸­ï¼š1è¡¨ç¤ºå‰æ™¯ï¼Œ0è¡¨ç¤ºèƒŒæ™¯ï¼Œæˆ‘ä»¬ç”Ÿæˆä¸€ä¸ªç®€å•çš„trimap
    trimap = np.ones_like(mask, dtype=np.float32) * 128  # é»˜è®¤å…¨éƒ¨ä¸ºunknown
    trimap[mask > 127] = 255  # å‰æ™¯åŒºåŸŸ
    trimap[mask < 128] = 0    # èƒŒæ™¯åŒºåŸŸ
    
    # ä¸ºäº†æœ‰unknownåŒºåŸŸï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è¾¹ç•Œåˆ›å»ºä¸€äº›unknownåŒºåŸŸ
    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç›´æ¥ä½¿ç”¨å…¨unknown
    trimap[:] = 128  # å…¨éƒ¨è®¾ä¸ºunknownï¼Œè®©æ¨¡å‹åœ¨æ•´ä¸ªåŒºåŸŸè¿›è¡Œé¢„æµ‹
    
    trimap_tensor = torch.from_numpy(trimap).unsqueeze(0).unsqueeze(0)  # [1, 1, H, W]
    trimap_tensor = DataProcessor.normalize_trimap(trimap_tensor)
    
    return mask_tensor.to(device), trimap_tensor.to(device)


# create_mask_promptå‡½æ•°å·²ç§»é™¤ï¼Œç›´æ¥ä½¿ç”¨binary maskä½œä¸ºmaskæç¤º


def inference_single_image(model, image_path, mask_path=None, output_path=None, visualize=False):
    """
    å¯¹å•å¼ å›¾åƒè¿›è¡Œæ¨ç†
    
    Args:
        model: åŠ è½½çš„æ¨¡å‹
        image_path: è¾“å…¥å›¾åƒè·¯å¾„
        mask_path: binary maskè·¯å¾„ï¼ˆå¯é€‰ï¼‰
        output_path: è¾“å‡ºè·¯å¾„
        visualize: æ˜¯å¦å¯è§†åŒ–ç»“æœ
        
    Returns:
        pred_alpha: é¢„æµ‹çš„alpha matte
    """
    print(f"Processing: {image_path}")
    
    # åŠ è½½å›¾åƒå’Œmask
    image_tensor, original_size = load_image(image_path)
    mask_tensor, trimap_tensor = load_mask(mask_path)
    
    # åˆ›å»ºsample_mapï¼ˆä»trimapç”Ÿæˆï¼Œç”¨äºæŒ‡å®šè®¡ç®—æŸå¤±çš„åŒºåŸŸï¼‰
    sample_map = DataProcessor.create_sample_map(trimap_tensor)
    
    # ç›´æ¥ä½¿ç”¨binary maskä½œä¸ºmaskæç¤º
    mask_prompt = mask_tensor
    
    # æ¨ç†
    with torch.no_grad():
        # è®¾ç½®æ¨¡å‹è¾“å…¥
        model.set_input(image_tensor, trimap_tensor, mask_inputs=mask_prompt, sample_map=sample_map)
        
        # å‰å‘ä¼ æ’­
        model.forward()
        
        # è·å–é¢„æµ‹ç»“æœ
        pred_alpha = model.pred_mask
        
        # è½¬æ¢ä¸ºnumpy
        pred_alpha_np = pred_alpha.squeeze().cpu().numpy()
        
        # è°ƒæ•´å›åŸå§‹å°ºå¯¸
        pred_alpha_np = cv2.resize(pred_alpha_np, (original_size[1], original_size[0]), 
                                  interpolation=cv2.INTER_LINEAR)
        
        # ç¡®ä¿å€¼åœ¨[0,1]èŒƒå›´å†…
        pred_alpha_np = np.clip(pred_alpha_np, 0, 1)
    
    # ä¿å­˜ç»“æœ
    if output_path:
        # è½¬æ¢ä¸º0-255èŒƒå›´å¹¶ä¿å­˜
        alpha_save = (pred_alpha_np * 255).astype(np.uint8)
        cv2.imwrite(output_path, alpha_save)
        print(f"âœ… Result saved to: {output_path}")
    
    # å¯è§†åŒ–
    if visualize:
        visualize_result(image_path, mask_path, pred_alpha_np, output_path)
    
    return pred_alpha_np


def visualize_result(image_path, mask_path, pred_alpha, output_path=None):
    """
    å¯è§†åŒ–æ¨ç†ç»“æœ
    
    Args:
        image_path: åŸå§‹å›¾åƒè·¯å¾„
        mask_path: binary maskè·¯å¾„
        pred_alpha: é¢„æµ‹çš„alpha
        output_path: è¾“å‡ºè·¯å¾„ï¼ˆç”¨äºç”Ÿæˆå¯è§†åŒ–æ–‡ä»¶åï¼‰
    """
    fig, axes = plt.subplots(1, 4, figsize=(20, 5))
    
    # åŸå§‹å›¾åƒ
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    axes[0].imshow(image)
    axes[0].set_title('Input Image')
    axes[0].axis('off')
    
    # Binary Mask
    if mask_path and os.path.exists(mask_path):
        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
        axes[1].imshow(mask, cmap='gray')
        axes[1].set_title('Binary Mask Input')
    else:
        axes[1].text(0.5, 0.5, 'No Mask', ha='center', va='center', transform=axes[1].transAxes)
        axes[1].set_title('Mask (Not Provided)')
    axes[1].axis('off')
    
    # é¢„æµ‹çš„Alpha
    axes[2].imshow(pred_alpha, cmap='gray')
    axes[2].set_title('Predicted Alpha')
    axes[2].axis('off')
    
    # åˆæˆç»“æœï¼ˆå¦‚æœå¯èƒ½ï¼‰
    try:
        # è°ƒæ•´alphaåˆ°åŸå§‹å›¾åƒå°ºå¯¸
        alpha_resized = cv2.resize(pred_alpha, (image.shape[1], image.shape[0]))
        alpha_3ch = np.stack([alpha_resized] * 3, axis=2)
        composite = image * alpha_3ch
        axes[3].imshow(composite.astype(np.uint8))
        axes[3].set_title('Composite Result')
    except:
        axes[3].text(0.5, 0.5, 'Composite Error', ha='center', va='center', transform=axes[3].transAxes)
        axes[3].set_title('Composite')
    axes[3].axis('off')
    
    plt.tight_layout()
    
    # ä¿å­˜å¯è§†åŒ–ç»“æœ
    if output_path:
        vis_path = output_path.replace('.png', '_visualization.png').replace('.jpg', '_visualization.png')
        plt.savefig(vis_path, dpi=150, bbox_inches='tight')
        print(f"ğŸ“Š Visualization saved to: {vis_path}")
    
    plt.show()


def batch_inference(model, input_dir, mask_dir=None, output_dir=None, visualize=False):
    """
    æ‰¹é‡æ¨ç†
    
    Args:
        model: åŠ è½½çš„æ¨¡å‹
        input_dir: è¾“å…¥å›¾åƒç›®å½•
        mask_dir: binary maskç›®å½•ï¼ˆå¯é€‰ï¼‰
        output_dir: è¾“å‡ºç›®å½•
        visualize: æ˜¯å¦å¯è§†åŒ–
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
    
    # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
    image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff']
    image_files = []
    for ext in image_extensions:
        image_files.extend(glob.glob(os.path.join(input_dir, ext)))
    
    if not image_files:
        print(f"âŒ No image files found in {input_dir}")
        return
    
    print(f"ğŸ“ Found {len(image_files)} images to process")
    
    # æ‰¹é‡å¤„ç†
    for image_path in tqdm(image_files, desc="Processing images"):
        # è·å–å›¾åƒåç§°
        img_name = os.path.basename(image_path)
        name_without_ext = os.path.splitext(img_name)[0]
        
        # æ„å»ºmaskè·¯å¾„
        mask_path = None
        if mask_dir:
            mask_path = os.path.join(mask_dir, img_name)
            if not os.path.exists(mask_path):
                # å°è¯•å…¶ä»–æ‰©å±•å
                for ext in ['.png', '.jpg', '.jpeg']:
                    mask_candidate = os.path.join(mask_dir, name_without_ext + ext)
                    if os.path.exists(mask_candidate):
                        mask_path = mask_candidate
                        break
                else:
                    mask_path = None
        
        # æ„å»ºè¾“å‡ºè·¯å¾„
        if output_dir:
            output_path = os.path.join(output_dir, name_without_ext + '.png')  # ç›´æ¥ä½¿ç”¨åŸæ–‡ä»¶å
        else:
            output_path = None
        
        try:
            # æ‰§è¡Œæ¨ç†
            inference_single_image(model, image_path, mask_path, output_path, visualize=False)
        except Exception as e:
            print(f"âŒ Error processing {img_name}: {e}")
    
    print(f"âœ… Batch processing completed. Results saved to: {output_dir}")


def main():
    print("ğŸš€ Starting SAM Prompt Adapter Inference")
    print(f"ğŸ“„ Checkpoint: {CHECKPOINT_PATH}")
    print(f"ğŸ“ Input: {INPUT_PATH}")
    print(f"ğŸ­ Mask: {MASK_PATH}")
    print(f"ğŸ’¾ Output: {OUTPUT_PATH}")
    
    # åŠ è½½æ¨¡å‹
    model = load_model(CHECKPOINT_PATH)
    
    if BATCH_MODE or os.path.isdir(INPUT_PATH):
        # æ‰¹é‡å¤„ç†
        batch_inference(
            model=model,
            input_dir=INPUT_PATH,
            mask_dir=MASK_PATH,
            output_dir=OUTPUT_PATH,
            visualize=VISUALIZE
        )
    else:
        # å•å¼ å¤„ç†
        inference_single_image(
            model=model,
            image_path=INPUT_PATH,
            mask_path=MASK_PATH,
            output_path=OUTPUT_PATH,
            visualize=VISUALIZE
        )
    
    print("ğŸ‰ Inference completed!")


if __name__ == '__main__':
        main()